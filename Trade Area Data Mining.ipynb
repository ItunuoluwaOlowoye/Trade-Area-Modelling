{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade Area Modeling\n",
    "According to [Market Business News](https://marketbusinessnews.com/financial-glossary/trade-area-definition-meaning/), a trade (or market) area is the geographical area  where all or most of a business' sales volume occurs. It can be used to make decisions about the optimum location for business growth and expansion.\n",
    "\n",
    "There are different trade area model and in this use case, we will be using the widely accepted [Huff model](https://en.wikipedia.org/wiki/Huff_model). The Huff model uses the **distance** between customers and existing business locations, or locations that are planned for the future, along with the **attractiveness of those locations**, to determine the likelihood of customers visiting those locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study\n",
    "A beauty and personal care brand (we'll call them Lotionfy) has thrived as an exclusively online business for the past two years. They serve a predominantly US market, have done some market research and this is what they have discovered.\n",
    "1. While the e-commerce share of total retail sales continues to rise, in-store shopping is still the preferred method for most US cusstomers and makes up a very large chunk of total retail sales. [[Reference](https://capitaloneshopping.com/research/online-vs-in-store-shopping-statistics/)]\n",
    "2. More online shoppers are opting to pick up orders in-store and shoppers are generally returning to stores; retailers are reaching customers with small-format stores to encourage impulse purchases and reach new markets, among other benefits[[Reference](https://www2.deloitte.com/us/en/pages/consulting/articles/q1-2023-consumer-trends-report.html)]\n",
    "3. Online businesses are developing physical footprints to keep their customers loyal and give them additional touchpoints for the brand; customers themselves now prefer omnichannel experiences [[Reference](https://www.inc.com/rebecca-deczynski/the-future-of-retail-isnt-direct-to-consumer-brands-embracing-brick-mortar-2023.html)]\n",
    "\n",
    "In light of these, Lotionfy has decided to build their physical presence. To start, they have also decided to leverage partnerships with already existing [top department stores](https://www.junglescout.com/wp-content/uploads/2023/09/Jungle-Scout-Consumer-Trends-Report-Q3-2023.pdf) in the US like Walmart, Target, Kohl's etc. They will be leveraging the small-format store approach by drilling down to neighborhood locations.\n",
    "They have decided to use a trade area model to know which cities to focus on and which store locations should be their areas of focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Mining a customer base for the brand](#Mining-a-customer-base-for-the-brand)\n",
    "    * [Modules and Clients](#Modules-and-Clients)\n",
    "    * [Creating cities](#creating-cities)\n",
    "    * [Creating city neighborhoods](#creating-city-neighborhoods)\n",
    "    * [Obtaining neighborhood population](#obtaining-neighborhood-population)\n",
    "    * [Generating customer population](#generating-customer-population)\n",
    "    * [Find prime city with most customers](#find-prime-city-with-most-customers)\n",
    "2. [Store Locations](#store-locations)\n",
    "    * [Finding stores](#finding-stores)\n",
    "    * [Distance matrix and travel time indices](#distance-matrix-and-travel-time-indices)\n",
    "        * [Distance matrix](#distance-matrix)\n",
    "        * [Travel time indices](#travel-time-indices)\n",
    "    * [Store preferences](#store-preferences)\n",
    "        * [Travel time](#travel-time)\n",
    "        * [Notable commercial businesses](#notable-commercial-businesses)\n",
    "        * [Highways](#highways)\n",
    "        * [Design index](#design-index)\n",
    "        * [Accessibility](#accessibility)\n",
    "        * [Parking spaces](#parking-spaces)\n",
    "        * [Store size](#store-size)\n",
    "        * [Code](#code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining a customer base for the brand\n",
    "To begin with, the [Statistical Atlas](https://statisticalatlas.com/United-States/Overview) website is scraped to get the list of some cities in the US. These cities are further scraped to get the list of city neighborhoods and their population. The map boundaries of these neighborhoods are gotten from [Zillow](https://catalog.data.gov/dataset/neighborhoods-us-2017-zillow-segs1).\n",
    "\n",
    "It is quite unlikely for the entire population of a neighborhood to be part of the customer base so the customer population in the neighborhoods will be randomly generated, ensuring that it is no more than the total population. This is to avoid using confidential data from the business.\n",
    "\n",
    "_Please note that this scraping was done in November/December 2023. After the time of this scraping, the website UI may have been changed by the web developers and the code may need to be changed accordingly._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules and Clients\n",
    "Here, we load all Python modules/packages needed for the data mining. API keys were protected and stored using the dotenv module. The neighborhood map boundaries are stored using the Mercator projection so we also created a user-defined function to convert Mercator projections to lat-lng coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "from shapely import wkt, Polygon, MultiPolygon\n",
    "from timezonefinder import TimezoneFinder\n",
    "from geopy.geocoders import Nominatim\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import googlemaps\n",
    "import datetime\n",
    "import requests\n",
    "import polyline\n",
    "import random\n",
    "import pyproj\n",
    "import pytz\n",
    "import utm\n",
    "import us\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "load_dotenv()\n",
    "# create a random seed for reproducibility\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iolowoye\\Anaconda3\\envs\\streamlit\\lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "c:\\Users\\iolowoye\\Anaconda3\\envs\\streamlit\\lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    }
   ],
   "source": [
    "# create Nominatim client\n",
    "nominatim_locator = Nominatim(user_agent='trade_area_model', timeout=30)\n",
    "# initialize googlemaps client - use your own key\n",
    "gmaps = googlemaps.Client(key=os.getenv('google_api_key'))\n",
    "# store OpenCage API key - use your own key\n",
    "opencage_api_key = os.getenv('opencage_api_key')\n",
    "# create timezone finder\n",
    "tf = TimezoneFinder()\n",
    "\n",
    "# convert from Mercator projection\n",
    "mercator = pyproj.Proj(init='epsg:3857')\n",
    "wgs84 = pyproj.Proj(init='epsg:4326')\n",
    "def convert_to_latlon(geometry):\n",
    "    shape = wkt.loads(geometry)\n",
    "    if shape.geom_type == 'Polygon':\n",
    "        return pyproj.transform(mercator, wgs84, *shape.exterior.coords.xy)\n",
    "    elif shape.geom_type == 'MultiPolygon':\n",
    "        new_polygons = []\n",
    "        for polygon in shape.geoms:\n",
    "            lng, lat = pyproj.transform(mercator, wgs84, *polygon.exterior.xy)\n",
    "        new_polygons.append(Polygon(zip(lng, lat)))\n",
    "        return MultiPolygon(new_polygons)\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating cities\n",
    "Here, we scrape a list of cities in the US (not all cities) and their states. We also use Nominatim to get the lat-lng coordinates of these cities. The results are stored in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to store html list elements, and major cities and places\n",
    "html_list, city_list = [], []\n",
    "# initiate the http request to the website for scraping\n",
    "url = f'https://statisticalatlas.com/United-States/Overview'\n",
    "response = requests.get(url)\n",
    "# if the http response is successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') # parse html\n",
    "    # find all div elements with the table class\n",
    "    contents = soup.find_all('div', class_='info-table-contents-div')\n",
    "    # if the hyperlink confirms that it is a city, add it to the html list\n",
    "    for content in contents:\n",
    "        hyperlink = content.find('a').get('href')\n",
    "        if 'place' in hyperlink:\n",
    "            cities = content.text.strip()\n",
    "            html_list.append(cities)\n",
    "            # add the cities in each html list element to the city list\n",
    "            for cities in html_list:\n",
    "                cities = cities.split(', ')\n",
    "                city_list += cities\n",
    "else: # if the http request failed\n",
    "    print(f'Request failed: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for cities and states\n",
    "city_state_dict = {}\n",
    "# for each city\n",
    "for city in city_list:\n",
    "    # geocode its location\n",
    "    location = nominatim_locator.geocode(city).raw\n",
    "    latitude, longitude = location['lat'], location['lon']\n",
    "    state = location['display_name'].split(', ')[-2]\n",
    "    # add the city, state, lat and long to the dictionary\n",
    "    city_state_dict[city] = [state, latitude, longitude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the customer cities\n",
    "customer_cities = pd.DataFrame(city_state_dict.items(), columns=['city', 'state'])\n",
    "customer_cities[['state','latitude','longitude']] = customer_cities['state'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save customer cities data\n",
    "customer_cities.to_csv('data/customers/customer_cities.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating city neighborhoods\n",
    "Here, the neighborhoods for all cities are scraped and cleaned. We restrict neighborhoods to only those who have available map boundaries and we convert these boundaries to lat-lng coordinates using the Mercator conversion function created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a city location column\n",
    "customer_cities['location'] = customer_cities[['city','state']].agg(lambda x: ', '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store the list of neighborhoods in each city\n",
    "city_neighborhoods = {}\n",
    "\n",
    "# iterating through each row\n",
    "for row in customer_cities.index:\n",
    "    # store the state name, city and location in variables\n",
    "    # rename certain cities based on the names used in the website we are web scraping from\n",
    "    state = customer_cities.loc[row,'state'].replace(' ','-')\n",
    "    city = customer_cities.loc[row,'city'].replace(' ','-')\n",
    "    if city == 'Boise' and state == 'Idaho': city = 'Boise-City'\n",
    "    elif city == 'Indianapolis' and state == 'Indiana': city = 'Indianapolis-city-(balance)'\n",
    "    elif city == 'Louisville' and state == 'Kentucky': city = 'Louisville/Jefferson-County-metro-government-(balance)'\n",
    "    elif city == 'Nashville-Davidson' and state == 'Tennessee': city = 'Nashville-Davidson-metropolitan-government-(balance)'\n",
    "    location = customer_cities.loc[row,'location']\n",
    "    # create the url to scrape from and initiate a http request\n",
    "    url = f'https://statisticalatlas.com/place/{state}/{city}/Overview'\n",
    "    response = requests.get(url)\n",
    "    # if the http response is successful\n",
    "    if response.status_code == 200:\n",
    "        try: # try this\n",
    "            soup = BeautifulSoup(response.text, 'html.parser') # parse html\n",
    "            # find the div and class elements that should contain the list of neighborhoods\n",
    "            # confirm from the hyperlinks that it contains neighborhood data\n",
    "            contents = soup.find_all('div', class_='info-table-contents-div')\n",
    "            for content in contents:\n",
    "                hyperlink = content.find('a').get('href')\n",
    "                if 'neighborhood' in hyperlink:\n",
    "                    neighborhoods = content.text.strip()\n",
    "                    # store the neighborhood data as values in the dictionary with the location as keys\n",
    "                    city_neighborhoods[location] = neighborhoods\n",
    "        except Exception as error: # if there is an error, print the error and continue execution\n",
    "            print(url); print(str(error)); pass\n",
    "    else: # if the response failed, print the status code\n",
    "        print(\"Failed to get data, status code:\",response.status_code)\n",
    "    response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the locations (city and state) and neighborhoods\n",
    "city_communities = pd.DataFrame(list(city_neighborhoods.items()), columns=['location', 'neighborhoods'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the neighborhoods by the delimiter\n",
    "random.seed(random_seed)\n",
    "city_communities['neighborhoods'] = city_communities['neighborhoods'].str.split(', ')#\\\n",
    "    #.apply(lambda x: random.sample(x, min(30, len(x))))\n",
    "# explode the list of neighborhoods so each has its unique row\n",
    "city_communities = city_communities.explode('neighborhoods', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "city_communities = city_communities[~(city_communities.duplicated())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data with neighborhood boundaries\n",
    "zillow_neighborhoods = gpd.read_file('data\\ZillowNeighborhoods.gdb', layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "zillow_neighborhoods.drop_duplicates('RegionID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of US state names and abbr\n",
    "state_names_dict = {}\n",
    "for state in us.STATES_AND_TERRITORIES:\n",
    "    state_names_dict[state.abbr] = state\n",
    "state_names_dict['DC'] = 'District of Columbia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert state abbr to names in neighborhood boundaries data\n",
    "zillow_neighborhoods['State'] = zillow_neighborhoods['State'].replace(state_names_dict)\n",
    "# add county name for Nashville\n",
    "zillow_neighborhoods.loc[zillow_neighborhoods['City'].str.contains('Nashville'), 'City'] = 'Nashville-Davidson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for neighborhood boundaries from Zillow\n",
    "neighborhood_boundaries = pd.DataFrame([])\n",
    "neighborhood_boundaries[['neighborhoods', 'geometry']] = zillow_neighborhoods[['Name','geometry']]\n",
    "neighborhood_boundaries['location'] = zillow_neighborhoods[['City','State']].astype(str)\\\n",
    "    .apply(lambda x: ', '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iolowoye\\AppData\\Local\\Temp\\ipykernel_20336\\1484609798.py:20: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  lng, lat = pyproj.transform(mercator, wgs84, *polygon.exterior.xy)\n"
     ]
    }
   ],
   "source": [
    "# add neighborhood boundaries to city communities/neighborhoods data\n",
    "city_communities = city_communities.merge(neighborhood_boundaries, how='inner', on=['location','neighborhoods'])\n",
    "# convert geometry from Mercator projection to lng-lat coordinates\n",
    "city_communities['geometry'] = city_communities['geometry'].astype(str).apply(convert_to_latlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove neighborhoods without boundary polygons\n",
    "city_communities = city_communities[city_communities['geometry'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates and cities without listed neighborhoods, if any\n",
    "city_communities = city_communities[~(city_communities.duplicated()) & (city_communities['neighborhoods'] != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining neighborhood population\n",
    "Here, we scrape for the population within all neighborhoods. Some cities are bigger than others so for building the final customer base, we restrict the list of neighborhoods for each city to no more than thirty (30). Results are stored in csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Algonquin/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Bon-Air/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Bowman/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Brownsboro-Zorn/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Chickasaw/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Edgewood/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Hallmark/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Hikes-Point/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Iroquois-Park/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Kenwood-Hill/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Klondike/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Park-Duvalle/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Prestonia/Overview 200 [<Response [303]>]\n",
      "Failed to get data from  https://statisticalatlas.com/neighborhood/Kentucky/Louisville/Jefferson-County-metro-government-(balance)/Soutland-Park/Overview 200 [<Response [303]>]\n"
     ]
    }
   ],
   "source": [
    "# explicitly cast the population column as a string\n",
    "city_communities.loc[:,'population'] = ''\n",
    "\n",
    "# iterating through each neighborhood\n",
    "for row in city_communities.index:\n",
    "    # store state name, city, and neighborhood in variables\n",
    "    # replace certain cities based on the names in the website we are web scraping from\n",
    "    location = city_communities.loc[row,'location'].replace(' ','-').split(',-')\n",
    "    city, state = location[0], location[1]\n",
    "    if city == 'Boise' and state == 'Idaho': city = 'Boise-City'\n",
    "    elif city == 'Indianapolis' and state == 'Indiana': city = 'Indianapolis-city-(balance)'\n",
    "    elif city == 'Louisville' and state == 'Kentucky': city = 'Louisville/Jefferson-County-metro-government-(balance)'\n",
    "    elif city == 'Nashville-Davidson' and state == 'Tennessee': city = 'Nashville-Davidson-metropolitan-government-(balance)'\n",
    "    neighborhood = city_communities.loc[row,'neighborhoods'].replace(' ','-').replace(\"'\",'').replace('.','')\n",
    "    # create the site url and initiate the http request\n",
    "    url = f'https://statisticalatlas.com/neighborhood/{state}/{city}/{neighborhood}/Overview'\n",
    "    response = requests.get(url)\n",
    "    # if the request is successful\n",
    "    if response.status_code == 200 and response.history == []:\n",
    "        try: # try this\n",
    "            soup = BeautifulSoup(response.text, 'html.parser') # parse html\n",
    "            # find tr element containing the population number\n",
    "            population_html = soup.find_all('tr')[-2]\n",
    "            population = population_html.text.strip()\n",
    "            # store population in its cell\n",
    "            city_communities.loc[row, 'population'] = population\n",
    "        except Exception as error: # for exceptions, print the url, error, and continue execution\n",
    "            print(url, str(error), end=','); pass\n",
    "    else: # if the http response fails, print its url, status code, and history\n",
    "        print(\"Failed to get data from \", url, response.status_code, response.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows with empty population\n",
    "city_communities = city_communities[city_communities['population'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include a date key for versioning purposes\n",
    "city_communities.loc[:, 'table_date'] = datetime.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this data in csv\n",
    "city_communities.to_csv('data/all_city_communities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set each city to have no more than 30 neighborhoods/communities\n",
    "# this step can be skipped\n",
    "city_communities = city_communities.groupby('location', group_keys=False).apply(lambda x: x.sample(n=min(30, len(x)),\n",
    "                                                                                       random_state=random_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this data in csv\n",
    "city_communities.to_csv('data/city_communities.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating customer population\n",
    "Here, we randomly generate the customer base within all neighborhoods ensuring that the customer base is only within 10% to 60% of the total neighborhood population and the result is stored in a csv file. Remember that this is done to protect confidential information.\n",
    "\n",
    "In a business use case, the KYC data will be used to know the customer base of a brand and where they reside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column for customer population with reproducible random values less than the total population\n",
    "np.random.seed(random_seed)\n",
    "customer_communities = city_communities.copy()\n",
    "customer_communities['population'] = (customer_communities['population'].str.replace(',','').astype(float)\\\n",
    "                                      * np.random.uniform(0.1, 0.6, len(customer_communities))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this data in csv\n",
    "customer_communities.to_csv('data/customers/customer_communities.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find prime city with most customers\n",
    "Here, we find the city with the biggest customer base. The trade area model will be built within this city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location\n",
       "Phoenix, Arizona                    514656\n",
       "Los Angeles, California             332142\n",
       "New York, New York                  329176\n",
       "San Jose, California                326346\n",
       "Dallas, Texas                       282115\n",
       "Houston, Texas                      209220\n",
       "Boston, Massachusetts               208396\n",
       "Fresno, California                  197803\n",
       "Memphis, Tennessee                  186313\n",
       "Colorado Springs, Colorado          183442\n",
       "Las Vegas, Nevada                   162448\n",
       "Chicago, Illinois                   158123\n",
       "Arlington, Texas                    130606\n",
       "Raleigh, North Carolina             129287\n",
       "El Paso, Texas                      127429\n",
       "Miami, Florida                      124829\n",
       "Indianapolis, Indiana               122343\n",
       "Cleveland, Ohio                     108794\n",
       "Virginia Beach, Virginia            103860\n",
       "Long Beach, California               98455\n",
       "Philadelphia, Pennsylvania           89448\n",
       "San Diego, California                88608\n",
       "San Francisco, California            86354\n",
       "Denver, Colorado                     78322\n",
       "Seattle, Washington                  70038\n",
       "Austin, Texas                        67400\n",
       "Portland, Oregon                     55684\n",
       "Minneapolis, Minnesota               54694\n",
       "Oklahoma City, Oklahoma              53657\n",
       "Milwaukee, Wisconsin                 51578\n",
       "Sacramento, California               50651\n",
       "Detroit, Michigan                    48309\n",
       "Jacksonville, Florida                47325\n",
       "Washington, District of Columbia     40198\n",
       "Charlotte, North Carolina            38026\n",
       "San Antonio, Texas                   34645\n",
       "Columbus, Ohio                       33477\n",
       "Louisville, Kentucky                 31586\n",
       "Oakland, California                  29121\n",
       "Tucson, Arizona                      26107\n",
       "Baltimore, Maryland                  21664\n",
       "Wichita, Kansas                      20672\n",
       "Albuquerque, New Mexico              19772\n",
       "Kansas City, Missouri                19609\n",
       "Fort Worth, Texas                    17691\n",
       "Mesa, Arizona                        14498\n",
       "Atlanta, Georgia                     12343\n",
       "Omaha, Nebraska                       9501\n",
       "Tulsa, Oklahoma                       8679\n",
       "Nashville-Davidson, Tennessee         7351\n",
       "Name: population, dtype: int32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find city location with the most customers\n",
    "number_of_customers = customer_communities.groupby('location')['population'].sum()\\\n",
    "    .sort_values(ascending=False)\n",
    "number_of_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select prime city location\n",
    "prime_location = number_of_customers.index[0]\n",
    "# get community coordinates\n",
    "customer_communities['coord'] = customer_communities['geometry'].apply(lambda x: wkt.loads(x).centroid.xy)\n",
    "# select customer neighborhoods in the prime city location\n",
    "prime_city_neighborhoods = customer_communities.loc[customer_communities['location'] == prime_location,\n",
    "                                                ['location','neighborhoods']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Locations\n",
    "The following department stores in the prime location will be used as potential store locations: Walmart, Target, JCPenny, Kohl's and Marshalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding stores\n",
    "Here, we get some locations of the selected department stores using the Google Maps API and store the results in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address(geometry):\n",
    "    lat, lng = geometry.centroid.y, geometry.centroid.x\n",
    "    result = gmaps.reverse_geocode((lat, lng))\n",
    "    address = result[0]['formatted_address']\n",
    "    address = ', '.join(address.split(', ')[:2])\n",
    "    return address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nominatim_address(gmap_address):\n",
    "    results = gmaps.places(query=gmap_address)\n",
    "    geom = results['results'][0]['geometry']['location']\n",
    "    lat, lng = geom['lat'], geom['lng']\n",
    "    address = nominatim_locator.reverse((lat, lng)).raw['display_name']\n",
    "    return address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Target; 5715 N 19th Ave, Phoenix, AZ 85015, United States: Nominatim could not geocode query 'Target, 5715;1760, West Montebello Avenue, Alhambra, Phoenix, Maricopa County, Arizona, 85015, United States'\n",
      "\n",
      "Error with Target Grocery; 5715 N 19th Ave, Phoenix, AZ 85015, United States: Nominatim could not geocode query 'Target, 5715;1760, West Montebello Avenue, Alhambra, Phoenix, Maricopa County, Arizona, 85015, United States'\n",
      "\n",
      "Error with Kohl's; 34830 N Vly Pkwy, Phoenix, AZ 85086, United States: No data elements in server response. Check log and query location/tags.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for these department stores on GMaps\n",
    "# get the OSM features of these stores\n",
    "department_stores = pd.DataFrame()\n",
    "dept_store_list = [\"Walmart\", \"Target\", \"JCPenney\", \"Kohl's\", \"Marshalls\"]\n",
    "for name in dept_store_list:\n",
    "    query = f\"{name}, {prime_location}\"\n",
    "    results = gmaps.places(query=query)\n",
    "    for result in results['results']:\n",
    "        address = result['formatted_address']\n",
    "        name = result['name']\n",
    "        try:\n",
    "            nominatim_address = get_nominatim_address(address)\n",
    "            df = ox.features_from_address(nominatim_address, tags = {'shop':['department_store','supermarket']})\n",
    "            df.loc[:,['gmap_name', 'address']] = name, ', '.join(address.split(', ')[:2])\n",
    "            department_stores = pd.concat([department_stores, df], axis=0)\n",
    "        except Exception as error: \n",
    "            print(f'Error with {name}; {address}: {error}', end='\\n\\n'); pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all available department store names\n",
    "department_store_names = department_stores['name'].dropna().unique().tolist()\n",
    "# select all stores that contain the target store names\n",
    "selected_stores = [item for item in department_store_names if any(store in item for store in dept_store_list)]\n",
    "# keep stores with defined names and geopolygons\n",
    "department_stores = department_stores.loc[(department_stores['name'].isin(selected_stores)) &\n",
    "                                          (department_stores['geometry'].astype(str).str.contains('POLYGON')),\n",
    "                                          ['name','gmap_name','address', 'geometry','building:levels']]\n",
    "# remove duplicates\n",
    "department_stores = department_stores.sort_index().reset_index().drop_duplicates('osmid',keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the correct gmaps address of stores near to the gmap store that was also selected by OSM\n",
    "department_stores.loc[department_stores['name'] != department_stores['gmap_name'], 'address'] = \\\n",
    "    department_stores['geometry'].apply(get_address)\n",
    "# remove the gmap name from the dataframe\n",
    "department_stores.drop('gmap_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create store locations with name and address\n",
    "department_stores['store_location'] = department_stores[['name','address']].apply(lambda x:', '.join(x), axis=1)\n",
    "store_locations = department_stores['store_location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_stores['city_location'] = prime_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_stores.to_csv('data/stores/store_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance matrix and travel time indices\n",
    "Here, for each store location, we leverage the Google Maps API to estimate:\n",
    "1. The distance between the customer neighborhoods and the store\n",
    "2. The time spent in rush-hour traffic (at 5pm)\n",
    "3. The route/path from the neighborhoods to the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get timezone of prime location\n",
    "geocode_prime_location = gmaps.geocode(prime_location)\n",
    "prime_location_geom = geocode_prime_location[0]['geometry']['location']\n",
    "timezone = tf.timezone_at(lat=prime_location_geom['lat'], lng=prime_location_geom['lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store in store_locations:\n",
    "    # create column for store path, distance and time from neighborhood\n",
    "    prime_city_neighborhoods.loc[:, 'distance '+store] = ''\n",
    "    prime_city_neighborhoods.loc[:, 'time '+store] = ''\n",
    "    prime_city_neighborhoods.loc[:, 'path '+store] = ''\n",
    "    # iterating through each neighborhood\n",
    "    for row in prime_city_neighborhoods.index:\n",
    "        # store the customer location and its geolocation in variables\n",
    "        neighborhoods = prime_city_neighborhoods.loc[row, 'coord']\n",
    "        lng, lat = neighborhoods[0][0], neighborhoods[1][0]\n",
    "        # set gmaps departure time\n",
    "        try:\n",
    "            departure_time = datetime.datetime.now(pytz.timezone(timezone)).replace(hour=17, minute=0, second=0)\n",
    "            # get the path, distance and travel time and add them to the neighborhood df\n",
    "            geocode_result = gmaps.directions((lat, lng), store, mode='driving', traffic_model='pessimistic',\n",
    "                                              units='metric', departure_time=departure_time)\n",
    "        except:\n",
    "            day = datetime.date.today().day + 1\n",
    "            departure_time = datetime.datetime.now(pytz.timezone(timezone)).replace(day=day, hour=17, minute=0, second=0)\n",
    "            # get the path, distance and travel time and add them to the neighborhood df\n",
    "            geocode_result = gmaps.directions((lat, lng), store, mode='driving', traffic_model='pessimistic',\n",
    "                                            units='metric', departure_time=departure_time)\n",
    "        dist_duration = geocode_result[0]['legs'][0]\n",
    "        distance = dist_duration['distance']['text']\n",
    "        time = dist_duration['duration_in_traffic']['text']\n",
    "        encoded_path = geocode_result[0]['overview_polyline']['points']\n",
    "        path = polyline.decode(encoded_path)\n",
    "        prime_city_neighborhoods.loc[row, 'distance '+store] = distance\n",
    "        prime_city_neighborhoods.loc[row, 'time '+store] = time\n",
    "        prime_city_neighborhoods.loc[row, 'path '+store] = str(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to convert distances  to km\n",
    "def convert_to_km(distance_str):\n",
    "    value, unit = float(distance_str.split()[0]), distance_str.split()[1].lower()\n",
    "    unit_conversion_dict = {'km':1.0, 'm':0.001}\n",
    "    return value * unit_conversion_dict[unit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to convert time to minutes\n",
    "def convert_to_mins(time_str):\n",
    "    total_minutes = 0\n",
    "    time = time_str.split()\n",
    "    if 'hours' in time or 'hour' in time: total_minutes += (int(time[0]) * 60) + int(time[2])\n",
    "    elif 'mins' in time or 'min' in time: total_minutes += int(time[0])\n",
    "    return total_minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns for distance matrix\n",
    "distance_columns = ['neighborhoods'] + [col for col in prime_city_neighborhoods.columns if 'distance' in col]\n",
    "# create distance matrix\n",
    "distance_matrix = prime_city_neighborhoods.loc[:, distance_columns]\n",
    "# remove neighborhood prefixes and suffixes\n",
    "distance_matrix.columns = distance_matrix.columns.str.removeprefix('distance').str.removesuffix(prime_location).str.strip()\\\n",
    "    .str.removesuffix(',')\n",
    "# set neighborhoods as index\n",
    "distance_matrix.set_index('neighborhoods', inplace=True)\n",
    "# standardize units\n",
    "for col in distance_matrix.columns:\n",
    "    try: distance_matrix[col] = distance_matrix[col].map(convert_to_km)\n",
    "    except Exception as error: print(f'Could not convert for column {col}'); pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "distance_matrix.to_csv('data/stores/store_distances.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Travel time indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns for travel time\n",
    "time_columns = ['neighborhoods'] + [col for col in prime_city_neighborhoods if 'time' in col]\n",
    "# create travel time data\n",
    "travel_time = prime_city_neighborhoods.loc[:, time_columns]\n",
    "# remove neighborhood prefixes and suffixes\n",
    "travel_time.columns = travel_time.columns.str.removeprefix('time').str.removesuffix(prime_location).str.strip()\\\n",
    "    .str.removesuffix(',')\n",
    "# set neighborhoods as index\n",
    "travel_time.set_index('neighborhoods', inplace=True)\n",
    "# standardize units\n",
    "for col in travel_time.columns:\n",
    "    try: travel_time[col] = travel_time[col].map(convert_to_mins)\n",
    "    except Exception as error: print(f'Could not convert for column {col}'); pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data\n",
    "travel_time.to_csv('data/stores/traffic_time_mins.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns for travel time\n",
    "path_columns = ['neighborhoods'] + [col for col in prime_city_neighborhoods if 'path' in col]\n",
    "# create travel time data\n",
    "path_df = prime_city_neighborhoods.loc[:, path_columns]\n",
    "# remove neighborhood prefixes and suffixes\n",
    "path_df.columns = path_df.columns.str.removeprefix('path').str.removesuffix(prime_location).str.strip()\\\n",
    "    .str.removesuffix(',')\n",
    "# set neighborhoods as index\n",
    "path_df.set_index('neighborhoods', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data\n",
    "path_df.to_csv('data/stores/path_to_store.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store preferences\n",
    "These are metrics used to determine how attractive the store locations are to the customers. These include the travel time indices, number of notable commercial businesses, number of highways, design index, accessibility (how many drivable and walkable roads are there?), number of available parking spaces, and size of the store.\n",
    "\n",
    "**For store preference metrics other than travel time indices, the Open Street Map network is leveraged to obtain features within a 1km radius for each store.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Travel time\n",
    "The average travel time to each store from all neighborhoods is used as the traffic index to guage the willingness of customers to drive/bike/walk to the store. The higher the travel time, the less willing customers will be to go to the store, especially since this is not for a luxury brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notable commercial businesses\n",
    "These are named commercial districts/buildings near the store. The presence of these could indicate that customers would have other reasons to be in that location, especially during work hours (if they work in those buildings, they can stop by the store during break hours or after hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Highways\n",
    "These are major (motorway, trunk, primary, secondary, motorway link, trunk link, primary link, secondary link, motorway junction) roads near the store. If there are more major roads leading to the location, the easier it is to drive there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Design index\n",
    "The presence of these specific features were used to determine the design index: cafes, restaurants, fuel stations, taxi stops, atms, banks, hospitals, community centres, conference centres, events venues, exhibition venues, marketplaces, resorts, fitness centers, gardens, parks, beaches, office buildings, educational buildings, religious buildings, hotels, and tourist attractions.\n",
    "\n",
    "All of these make places more attractive to people in general, and customers could have other reasons to visit that store location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accessibility\n",
    "In this use case, this speaks to the presence of minor roads to drive through, bus stops, subway stations, walkways, and cycle roads. These features make it even easier to get to the location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parking spaces\n",
    "Each department store will have its own parking lot. The presence of another public parking space nearby will also be advantageous. If the store parking lot is full, there's less need to worry about where to park without attracting a parking fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store size\n",
    "The bigger the store, the more variety they would be able to sell, and the more likely they are to have everything on a customer's shopping list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create travel index of store\n",
    "store_preferences = round(travel_time.mean(numeric_only=True), 1).to_frame(name='avg_travel_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart Neighborhood Market, 115 E Dunlap Ave, Phoenix preferences completed\n",
      "Walmart Supercenter, 4747 E Cactus Rd, Phoenix preferences completed\n",
      "Target, 21001 N Tatum Blvd, Phoenix preferences completed\n",
      "Target, 7409 W Virginia Ave, Phoenix preferences completed\n",
      "Walmart Supercenter, 1825 W Bell Rd, Phoenix preferences completed\n",
      "Walmart Supercenter, 5250 W Indian School Rd, Phoenix preferences completed\n",
      "Target, 9830 W Lower Buckeye Rd, Tolleson preferences completed\n",
      "Kohl's, 3000-3010 S 99th Ave, Tolleson preferences completed\n",
      "Walmart Supercenter, 3721 E Thomas Rd, Phoenix preferences completed\n",
      "Walmart Supercenter, 6145 N 35th Ave, Phoenix preferences completed\n",
      "Walmart Supercenter, 7575 W Lower Buckeye Rd, Phoenix preferences completed\n",
      "Walmart Supercenter, 2020 N 75th Ave, Phoenix preferences completed\n",
      "Target, 4722 E Ray Rd #3, Phoenix preferences completed\n",
      "Walmart Supercenter, 6150 S 35th Ave, Phoenix preferences completed\n",
      "Kohl's, 17232 N 19th Ave, Phoenix preferences completed\n",
      "Walmart Supercenter, 18551 N 83rd Ave &, W Union Hills Dr preferences completed\n",
      "Target, 8055 W Bell Rd, Peoria preferences completed\n",
      "Target, 10404 N 43rd Ave, Glendale preferences completed\n",
      "Target, 4515 E Thomas Rd, Phoenix preferences completed\n",
      "Kohl's, 21001 N Tatum Blvd, Phoenix preferences completed\n",
      "Walmart Supercenter, 1607 W Bethany Home Rd, Phoenix preferences completed\n",
      "Target, 5715 N 19th Ave, Phoenix preferences completed\n",
      "Target, 2727 W Agua Fria Fwy, Phoenix preferences completed\n",
      "Walmart Supercenter, 9600 N Metro Pkwy W, Phoenix preferences completed\n",
      "Walmart Supercenter, 2501 W Happy Valley Rd Suite 34, Phoenix preferences completed\n",
      "Marshalls, 4729 E Ray Rd, Phoenix preferences completed\n",
      "Target, 2140 E Baseline Rd, Phoenix preferences completed\n",
      "Walmart Neighborhood Market, 2435 E Baseline Rd, Phoenix preferences completed\n",
      "Marshalls, 21001 N Tatum Blvd, Phoenix preferences completed\n",
      "Marshalls, 10130 W McDowell Rd, Avondale preferences completed\n"
     ]
    }
   ],
   "source": [
    "# create a list of major highway tags\n",
    "major_highways = ['motorway', 'trunk', 'primary', 'secondary', 'motorway_link', 'trunk_link',\n",
    "                  'primary_link', 'secondary_link', 'motorway_junction']\n",
    "\n",
    "# store the city and state in variables\n",
    "store_city = prime_location.split(', ')[0].replace(' ','-')\n",
    "store_state = prime_location.split(', ')[1].replace(' ','-')\n",
    "\n",
    "# create placeholder columns\n",
    "store_preferences.loc[:, 'business_communities'] = 0\n",
    "store_preferences.loc[:, 'highways'] = 0\n",
    "store_preferences.loc[:, 'design'] = 0\n",
    "store_preferences.loc[:, 'accessibility'] = 0\n",
    "store_preferences.loc[:, 'parking_space'] = 1\n",
    "store_preferences.loc[:, 'store_size'] = 0.0\n",
    "\n",
    "# create dictionaries for the different dataframes\n",
    "businesses_dict, highways_dict, design_dict, road_dict, walkways_dict, parking_dict = ({} for _ in range(6))\n",
    "\n",
    "# iterate through the store locations\n",
    "for store in store_locations:\n",
    "    \n",
    "    store_address = get_nominatim_address(store)\n",
    "    \"\"\" BUSINESS DISTRICTS \"\"\"\n",
    "    # find and count number of commercial landspace\n",
    "    try:\n",
    "        businesses_df = ox.features_from_address(store_address, tags={'landuse':'commercial'})\n",
    "        businesses_dict[store] = businesses_df\n",
    "        businesses = businesses_df['name'].nunique()\n",
    "        store_preferences.loc[store, 'business_communities'] = businesses\n",
    "    except: pass\n",
    "    \n",
    "    \"\"\" HIGHWAYS \"\"\"\n",
    "    # find and count number of highways\n",
    "    try:\n",
    "        highways_df = ox.features_from_address(store_address, tags={'highway':major_highways})\n",
    "        highways_dict[store] = highways_df\n",
    "        highways = highways_df['name'].nunique()\n",
    "        store_preferences.loc[store, 'highways'] = highways\n",
    "    except: pass\n",
    "    \n",
    "    \"\"\" MERCHANDIZING DESIGN \"\"\"\n",
    "    # find amenities and other neighborhood design touchpoints\n",
    "    try:\n",
    "        design_df = ox.features_from_address(store_address, tags={'amenity':['cafe','restaurant','fuel','taxi','atm','bank','hospital','community_centre','conference_centre','events_venue','exhibition_venue','marketplace'], \n",
    "                                                                  'leisure':['beach_resort','fitness_centre','garden','park'],\n",
    "                                                                  'natural':'beach', 'office':True,\n",
    "                                                                  'landuse':['education','religious'],\n",
    "                                                                  'tourism':['attraction','hotel']})\n",
    "        design_dict[store] = design_df\n",
    "        design = design_df.name.nunique()\n",
    "        store_preferences.loc[store, 'design'] = design\n",
    "    except: pass\n",
    "    \n",
    "    \"\"\" ACCESSIBILITY \"\"\"\n",
    "    # estimate accessibility\n",
    "    try:\n",
    "        road_df = ox.features_from_address(store_address, tags={'highway':['tertiary', 'bus_stop'], \n",
    "        'railway':'subway_entrance'})\n",
    "        road_dict[store] = road_df\n",
    "        roads = road_df.name.nunique()\n",
    "    except: roads = 0; pass\n",
    "    try:\n",
    "        walkways_df = ox.features_from_address(store_address, tags={'highway':['footway', 'track','cycleway']}).reset_index()\n",
    "        walkways_dict[store] = walkways_df\n",
    "        walkways = walkways_df.osmid.nunique()\n",
    "    except: walkways = 0; pass\n",
    "    store_preferences.loc[store, 'accessibility'] = roads + walkways\n",
    "    \n",
    "    \"\"\" PARKING SPACE \"\"\"\n",
    "    # create dataframe to store parking space\n",
    "    try:\n",
    "        parking_df = ox.features_from_address(store_address, tags = {'building':'parking', 'amenity':'parking'})\n",
    "        parking_df = parking_df.loc[parking_df['access'] == 'yes', :].reset_index().drop_duplicates('osmid')\n",
    "        try: parking_df['building:levels'] = parking_df['building:levels'].fillna(1).astype(int)\n",
    "        except: parking_df['building:levels'] = 1\n",
    "        parking_dict[store] = parking_df\n",
    "        parking = parking_df['building:levels'].sum(numeric_only=True) + 1\n",
    "        if parking > 0: store_preferences.loc[store, 'parking_space'] = parking\n",
    "    except: pass\n",
    "    \n",
    "    \"\"\" STORE SIZE \"\"\"\n",
    "    # select geometry\n",
    "    store_size_df = department_stores.loc[department_stores['store_location']==store, :]\n",
    "    try: geometry = list(store_size_df['geometry'].iloc[0].exterior.coords)\n",
    "    except: geometry = list(wkt.loads(store_size_df['geometry'].iloc[0]).exterior.coords)\n",
    "    # reverse the geometry position to be lat-long not long-lat\n",
    "    geometry_reverse = []\n",
    "    for coord in geometry:\n",
    "        latlon = (coord[1], coord[0])\n",
    "        geometry_reverse.append(latlon)\n",
    "    # extract utm zone\n",
    "    zone = utm.from_latlon(geometry_reverse[0][0], geometry_reverse[0][1])\n",
    "    zone_num = zone[2]\n",
    "    # get utm coordinates of store geometry\n",
    "    utm_zone = pyproj.Proj(f\"+proj=utm +zone={zone_num} +ellps=WGS84 +units=m\")\n",
    "    utm_coord = [utm_zone(latitude=lat, longitude=long) for lat, long in geometry_reverse]\n",
    "    # calculate the area of the selected retail store\n",
    "    area_m2 = Polygon(utm_coord).area\n",
    "    # count number of building levels\n",
    "    try: bld_lvls = int(store_size_df['building:levels'].iloc[0])\n",
    "    except: bld_lvls = 1\n",
    "    # calculate total store area\n",
    "    store_size_m2 = round(area_m2 * bld_lvls, 2)\n",
    "    # add to the store preferences data\n",
    "    store_preferences.loc[store, 'store_size'] = store_size_m2\n",
    "    print(f'{store} preferences completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preferences\n",
    "store_preferences.to_csv('data/stores/store_preferences.csv', index_label='store')\n",
    "# save features dataframes\n",
    "clear_df = pd.DataFrame([])\n",
    "for key in businesses_dict:\n",
    "    businesses_dict[key].loc[:, 'store'] = key\n",
    "    clear_df = pd.concat([clear_df, businesses_dict[key]], axis=0)\n",
    "    clear_df.to_csv('data/stores/business_communities.csv')\n",
    "clear_df = pd.DataFrame([])\n",
    "for key in highways_dict:\n",
    "    highways_dict[key].loc[:, 'store'] = key\n",
    "    clear_df = pd.concat([clear_df, highways_dict[key]], axis=0)\n",
    "    clear_df.to_csv('data/stores/highways.csv')\n",
    "clear_df = pd.DataFrame([])\n",
    "for key in design_dict:\n",
    "    design_dict[key]['store'] = key\n",
    "    clear_df = pd.concat([clear_df, design_dict[key]], axis=0)\n",
    "    clear_df.to_csv('data/stores/attractions.csv')\n",
    "clear_df = pd.DataFrame([])\n",
    "for key in road_dict:\n",
    "    road_dict[key]['store'] = key\n",
    "    clear_df = pd.concat([clear_df, road_dict[key]], axis=0)\n",
    "    clear_df.to_csv('data/stores/roads.csv')\n",
    "clear_df = pd.DataFrame([])\n",
    "for key in walkways_dict:\n",
    "    walkways_dict[key]['store'] = key\n",
    "    clear_df = pd.concat([clear_df, walkways_dict[key]], axis=0)\n",
    "    clear_df.to_csv('data/stores/walkways.csv', index=False)\n",
    "clear_df = pd.DataFrame([])\n",
    "for key in parking_dict:\n",
    "    parking_dict[key]['store'] = key\n",
    "    clear_df = pd.concat([clear_df, parking_dict[key]], axis=0)\n",
    "    clear_df.to_csv('data/stores/parking_spaces.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
